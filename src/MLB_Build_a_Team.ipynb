{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhNhzGlLKKmC"
   },
   "source": [
    "# Data and Analysis Plan: MLB Build a Team\n",
    "\n",
    "## Team 4\n",
    "- Tim Seymour (seymour.ti@northeastern.edu)\n",
    "\n",
    "## Project Goal:\n",
    "The goal of this project is to be able to predict how many wins any combination of players would have over a 162 game season. To do this, I build a classification model to estimate whether a team will win a given game. To generate a new data point, I will aggregate the statistics for a given lineup, rotation, and bullpen. For this new data point, I will use predict_proba to determine how likely that team is to win a given game - multiplying this value by 162 will give an estimate of how many wins that team would have assuming full health.\n",
    "- My original idea was to build a regression model with team win totals over a season as the target, but this severely limits the size of the data set. My preference was to use data from as recently as possible because baseball has changed dramatically throughout its history, so including data from past eras could water down the significance of home run rate and strikeout rate in predicting the winner of a single game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jW8klHfKKmG"
   },
   "source": [
    "<a id='data'></a>\n",
    "\n",
    "## Data\n",
    "I will obtain statistics from FanGraphs including:\n",
    "\n",
    "'HR', 'R', 'RBI', 'SB', 'BB%',\n",
    "       'K%', 'ISO', 'BABIP_x', 'AVG', 'OBP', 'SLG', 'wOBA', 'xwOBA', 'wRC+',\n",
    "       'BsR', 'Off', 'Def', 'WAR_x' (Hitter's war), 'SV',\n",
    "       'K/9', 'BB/9', 'HR/9', 'BABIP_y', 'LOB%', 'GB%', 'HR/FB', 'vFA (pi)',\n",
    "       'ERA', 'xERA', 'FIP', 'xFIP', 'WAR_y' (Pitcher's war)\n",
    "\n",
    "There are considerations to be made about the inclusion of several of these statistics with many relying on luck which should not be projected over a season sample size. This will have a large effect on my feature selection process - stats used should be under team and player control. For example, BABIP or Batting Average on Balls in Play is a metric that will vary game to game and likely has more to do with luck than skill. While exit velocity and defensive alignment will have an impact, it is more practical to use a different statistic for generalization purposes. \n",
    "\n",
    "A stat like Saves also implies a win, so this would lead to overfitting. I will analyze the choices that I make in the feature selection process as I go. One other consideration is that it would be preferable to include at least one statistic representing each facet of the game - Def and BsR are the only defensive and baserunning advanced statistics that are kept on a game to game basis, so provided they have even a 2-3 game impact over a season they will be included in the model.\n",
    "\n",
    "As for the inclusion of pitching and hitting statistics, I will make it a priority to include statistics that do not correlate strongly with each other (avoiding autocorrelation) to ensure the model has diverse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRjneplRKKmI"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import plotly\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from jupyter_dash import JupyterDash\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash import dash_table\n",
    "from dash.dependencies import Input, Output, State\n",
    "import json\n",
    "from dash.exceptions import PreventUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PXmPcOI16sA"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2pRlF2dKKmH"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7o_bNgRKKmJ"
   },
   "source": [
    "Scraping team data from 2015 and later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpPAtNtFPnIN"
   },
   "outputs": [],
   "source": [
    "def date_to_str(date):\n",
    "    '''\n",
    "    Args:\n",
    "        date (datetime): datetime object for the day of the season\n",
    "\n",
    "    Returns:\n",
    "        str: string representation of the given date\n",
    "\n",
    "    '''\n",
    "    month = str(date.month)\n",
    "    day = str(date.day)\n",
    "    if date.day <= 9:\n",
    "        day = str(0) + day\n",
    "    if date.month <= 9:\n",
    "        month = str(0) + month\n",
    "    return str(date.year) + '-' + month + '-' + day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include advanced stats on a game by game basis, I found FanGraphs to have the best combinations of metrics that would be helpful in predicting the winner of a single game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0jcChFV-GWf"
   },
   "source": [
    "Scraping process takes ~20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJAClzs3NolC"
   },
   "outputs": [],
   "source": [
    "def collect_team_data():\n",
    "    '''\n",
    "    Scrapes FanGraphs data from each day between April 1, 2015 and today's date\n",
    "\n",
    "    Returns:\n",
    "        hit (pd.DataFrame) contains hitting stats with each record representing one game for a team\n",
    "        pit (pd.DataFrame) contains pitching stats with each record representing one game for a team\n",
    "    '''\n",
    "    # beginning of sample is 2015\n",
    "    first_date = datetime.datetime(year = 2015, month = 4, day = 1)\n",
    "    # When date reaches last date, date resets to first_date (plus one year)\n",
    "    last_date = datetime.datetime(year = 2015, month = 10, day = 3)\n",
    "    date = datetime.datetime(year = 2015, month = 4, day = 1)\n",
    "    # collects team hitting stats for each day\n",
    "    hit = pd.DataFrame()\n",
    "    # collects team pitching stats for each day\n",
    "    pit = pd.DataFrame()\n",
    "    # sustainable way of changing year without change in code\n",
    "    while (date < datetime.datetime.now()):\n",
    "        date_str = date_to_str(date)\n",
    "        print(date_str)\n",
    "        # scrape hitting data\n",
    "        hit_df = pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={date.year}&month=1000&season1={date.year}&ind=0&team=0%2Cts&rost=0&age=0&filter=&players=0&startdate={date_str}&enddate={date_str}')\n",
    "        # getting rid of the final row with non-numeric data\n",
    "        hit_df = hit_df[16][:-1]\n",
    "        hit_df[('temp', 'Date')] = date_str\n",
    "        hit_df.columns = hit_df.columns.droplevel(0)\n",
    "        if len(hit_df['#']) > 1:\n",
    "            hit = hit.append(hit_df)\n",
    "        # scrape pitching data\n",
    "        pit_df = pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=8&season={date.year}&month=1000&season1={date.year}&ind=0&team=0%2Cts&rost=0&age=0&filter=&players=0&startdate={date_str}&enddate={date_str}')\n",
    "        # getting rid of the final row with non-numeric data\n",
    "        pit_df = pit_df[16][:-1]\n",
    "        pit_df[('temp', 'Date')] = date_str\n",
    "        pit_df.columns = pit_df.columns.droplevel(0)\n",
    "        if len(pit_df['#']) > 1:\n",
    "            pit = pit.append(pit_df)\n",
    "        if (date < last_date):\n",
    "            date += datetime.timedelta(days = 1)\n",
    "        else:\n",
    "            print(date.year)\n",
    "            last_date = datetime.datetime(year = last_date.year + 1, month = last_date.month, day = last_date.day)\n",
    "            first_date = datetime.datetime(year = first_date.year + 1, month = first_date.month, day = first_date.day)\n",
    "            date = first_date\n",
    "    return hit, pit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit, pit = collect_team_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2hDzrOPUpCr"
   },
   "source": [
    "# Shortened Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPXJyz70Acrm"
   },
   "outputs": [],
   "source": [
    "all_stats = pd.read_csv(\"C:/Users/timse/OneDrive/Documents/MLB Project/MLB-Build-a-Team/static_data/daily_stats.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEwQsShZQ9Y4"
   },
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cleaning process consisted of changing the data type of each numerical column from a string to a float. I also dropped columns that did not have values for most game logs ('xwOBA', 'xERA'), columns that do not have anything to do with whether a game is won ('Team', 'G', 'Date', 'GS'), and columns that were disqualified as valid predictors ('PA', 'R', 'L', 'SV', 'IP', 'RBI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuIfINs9DX3b"
   },
   "outputs": [],
   "source": [
    "# These columns have only null values for single games\n",
    "all_stats.drop(columns = ['xwOBA', 'xERA'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8RZd09oKKmO"
   },
   "outputs": [],
   "source": [
    "# turning every value in the dataframe into a float\n",
    "def string_to_num(string):\n",
    "    if(string == 'NA'):\n",
    "        return 'NA'\n",
    "    elif(type(string) == str):\n",
    "        if('%' in string):\n",
    "            string = string.replace('%', '')\n",
    "    return float(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evmS7T4DDTnB"
   },
   "outputs": [],
   "source": [
    "# applying the function to each column to ensure all data points are numerical\n",
    "for col in all_stats.columns:\n",
    "    if col not in ['Team', 'Date', 'GB']:\n",
    "        all_stats[col] = all_stats[col].apply(string_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIAmDhnqNuc0"
   },
   "source": [
    "### Subjective decision made to exclude RBI\n",
    "RBI is a statistic that is often outside of player control as it is highly dependent on how often runners are in scoring position for their at bats. It also could lead to overfitting as almost all runs result in an RBI and comparing RBIs to ERA would make for too obvious of a prediction. I have not excluded ERA to this point because, while it is a bit more within a player's control and is therefore more representative of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q8-9u00GLvL"
   },
   "outputs": [],
   "source": [
    "all_stats = all_stats.drop(columns = ['#_x', 'Team', 'G', 'PA', 'R',\n",
    "       'Date', '#_y', 'L', 'SV', 'GS', 'IP', 'RBI', 'Unnamed: 0'])\n",
    "# Only ~100 columns with null values\n",
    "all_stats.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3tCh3Rk1O0Q"
   },
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing was to separate the data into X and y with 'W' (win) being the target variable. The next step is to scale X using StandardScaler to normalize the dataset. I then store the factors used in scaling each feature in a dictionary to be used in scaling aggregated player data for prediction. Finally, I split the data randomly into train, test, and validation sets with 15% used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUUcr00b7E2J"
   },
   "outputs": [],
   "source": [
    "X = all_stats.drop(columns = ['W'])\n",
    "cols = X.columns\n",
    "y = all_stats['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-w7E51W1UIr"
   },
   "outputs": [],
   "source": [
    "#Scaling each column to be \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X = pd.DataFrame(X, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiW0puMO7G2S"
   },
   "outputs": [],
   "source": [
    "#Storing values used to scale each feature for manual normalization in later step\n",
    "feat_names = np.append(scaler.get_feature_names_out()[:-1], 'WAR')\n",
    "scales = pd.DataFrame({'Feature': feat_names, 'Unit Variance': scaler.scale_, 'Mean': scaler.mean_})\n",
    "scales.set_index('Feature', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5Yi79koTIwR5",
    "outputId": "d50f3680-ce51-4cdd-89ca-c59f5f099d29"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mXUNIg0JXg-"
   },
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.15, random_state=100)\n",
    "#Saving testing set for final model testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tr, y_tr, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tThlTTFRNVO"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to feature selection is to use recursive feature elimination with a logistic regression model to determine the optimal number of features. Once the results of each model is recorded, I plot the train and test accuracy against the number of features to evaluate for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-1r9Ag_XMdn"
   },
   "outputs": [],
   "source": [
    "# selects the X most important features to be used for the model\n",
    "def feature_selection(x_train, x_test, y_train, num_feats, print_bool):\n",
    "    '''\n",
    "    Args:\n",
    "          x_train (pd.DataFrame): training set\n",
    "          x_test (pd.DataFrame): testing set\n",
    "          y_train (pd.DataFrame): target variable\n",
    "          num_feats (int): number of features to select\n",
    "          print_bool (boolean): decides whether or not to print selected features\n",
    "\n",
    "    Returns:\n",
    "          X_train_selected (np.array) contains new training set with optimal features selected\n",
    "          X_test_selected (np.array) contains new testing set with optimal features selected\n",
    "\n",
    "  '''\n",
    "    # instantiate\n",
    "    select = RFE(DecisionTreeRegressor(random_state = 300), n_features_to_select = num_feats)\n",
    "    \n",
    "    # fit the RFE selector to the training data\n",
    "    select.fit(x_train, y_train)\n",
    "    \n",
    "    # transform training and testing sets so only the selected features are retained\n",
    "    X_train_selected = select.transform(x_train)\n",
    "    X_test_selected = select.transform(x_test)\n",
    "    \n",
    "    if print_bool:\n",
    "      # prints selected features/Sample Output\n",
    "      selected_features = [feature for feature, status in zip(x_train, select.get_support()) if status == True]\n",
    "      \n",
    "      print('Selected features:')\n",
    "      for feature in selected_features:\n",
    "            print(feature)\n",
    "\n",
    "    # returns selected features\n",
    "    return X_train_selected, X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEMwkwlWs1EP"
   },
   "outputs": [],
   "source": [
    "def calc_metrics(y_pred, y_actual, test_or_train, print_bool):\n",
    "    '''\n",
    "    input empty string for test_or_train if not relevant\n",
    "    otherwise put train or test with trailing space\n",
    "\n",
    "    Args:\n",
    "        y_pred (pd.DataFrame): values predicted by classifier\n",
    "        y_actual (pd.DataFrame): actual values\n",
    "        test_or_train (str): string that will precede metrics to distinguish training, testing measurements\n",
    "        print_bool (boolean): decides whether or not to print selected features\n",
    "\n",
    "    Returns:\n",
    "        results (Dict) contains metrics in format to be added to DataFrame\n",
    "    '''\n",
    "    a = accuracy_score(y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred)\n",
    "    prec = precision_score(y_actual, y_pred)\n",
    "    rec = recall_score(y_actual, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_actual, y_pred)\n",
    "    results = {f'{test_or_train}Accuracy': a, f'{test_or_train}Error': 1-a, f'{test_or_train}Precision': prec, f'{test_or_train}Recall': rec,\n",
    "          f'{test_or_train}F1 Score': f1, f'{test_or_train}AUC': auc}\n",
    "    if(print_bool):\n",
    "        print(f'{test_or_train}Error: {1 - a}')\n",
    "        print(f'{test_or_train}Accuracy: {a}')\n",
    "        print(f'{test_or_train}Precision: {prec}')\n",
    "        print(f'{test_or_train}Recall: {rec}')\n",
    "        print(f'{test_or_train}F1: {f1}')\n",
    "        print(f'{test_or_train}AUC: {auc}')\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbyrmYHmP1OF"
   },
   "source": [
    "Evaluating Logistic Regression Performance with different amounts of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA4zzzhJJuUF"
   },
   "outputs": [],
   "source": [
    "#list of dictionaries to make a DataFrame\n",
    "dicts = []\n",
    "# Evaluating models with 1 - 15 features\n",
    "for num_feat in range(1, 16, 1):\n",
    "    train, test = feature_selection(X_train, X_test, y_train, num_feat, False)\n",
    "    LR = LogisticRegression()\n",
    "    LR = LR.fit(train, y_train)\n",
    "    # Training predictions\n",
    "    y_pred_tr = LR.predict(train)\n",
    "    # Testing predictions\n",
    "    y_pred_test = LR.predict(test)\n",
    "    train_results = calc_metrics(y_pred_tr, y_train, 'Train ', False)\n",
    "    test_results = calc_metrics(y_pred_test, y_test, 'Test ', False)\n",
    "    train_results.update(test_results)\n",
    "    train_results['Num Features'] = num_feat\n",
    "    dicts.append(train_results)\n",
    "df = pd.DataFrame(dicts)\n",
    "#Repeating the process excluding result-oriented statistics (LOB%, BABIP)\n",
    "#list of dictionaries to make a DataFrame\n",
    "dicts = []\n",
    "# Evaluating models with 1 - 15 features\n",
    "for num_feat in range(1, 16, 1):\n",
    "    train, test = feature_selection(X_train.drop(columns = ['LOB%', 'BABIP_x', 'BABIP_y']), X_test.drop(columns = ['LOB%', 'BABIP_x', 'BABIP_y']), y_train, num_feat, False)\n",
    "    LR = LogisticRegression()\n",
    "    LR = LR.fit(train, y_train)\n",
    "    # Training predictions\n",
    "    y_pred_tr = LR.predict(train)\n",
    "    # Testing predictions\n",
    "    y_pred_test = LR.predict(test)\n",
    "    train_results = calc_metrics(y_pred_tr, y_train, 'Train ', False)\n",
    "    test_results = calc_metrics(y_pred_test, y_test, 'Test ', False)\n",
    "    train_results.update(test_results)\n",
    "    train_results['Num Features'] = num_feat\n",
    "    dicts.append(train_results)\n",
    "df_excluding = pd.DataFrame(dicts)\n",
    "# Renaming Columns for graphing purposes\n",
    "for col in df_excluding.columns:\n",
    "    df_excluding.rename(columns = {col: col + ' excluding LOB%, BABIP'}, inplace = True)\n",
    "df = pd.concat([df, df_excluding], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "4yjGX6W_Vc5V",
    "outputId": "f725bcd3-6948-402e-df9a-d5b1896b2564"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "zEoIpaD_J4uA",
    "outputId": "71b6769f-972f-47d1-c40b-cd3d4fde5be0"
   },
   "outputs": [],
   "source": [
    "px.line(df, x = 'Num Features', y = ['Train Accuracy', 'Test Accuracy', 'Train Accuracy excluding LOB%, BABIP', 'Test Accuracy excluding LOB%, BABIP'], title=\"Logistic Regression Accuracy with Different Numbers of Features\",\n",
    "            labels={ # replaces default labels by column name,\n",
    "                'value': \"Accuracy\", 'variable': 'Accuracy Type'}, range_y = [.88, .93])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN2qh-H22vs3"
   },
   "source": [
    "There is not much accuracy to be gained from using more than six or seven statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LusWIlnpaxa0"
   },
   "source": [
    "### Feature selection favors result-oriented statistics, unsurprisingly\n",
    "- LOB% signifies the percentage of baserunners that did not come around to score; this is a testament to the value of clutch hitting, although for this model this should not be factored in - timely hitting is more likely to be a result of luck\n",
    "- BABIP_y is the percentage of balls put in play that result in hits which has an element of luck and is not as representative of the strength of a team or player as batting average itself\n",
    "\n",
    "I have decided against using these metrics for the reasons I outline above and because as is seen in the graph, there is minimal gain in accuracy from their use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "116Ms7hpEt_Q",
    "outputId": "86b7d291-01cc-48fe-c000-85657b61330e"
   },
   "outputs": [],
   "source": [
    "feature_selection(X_train.drop(columns = ['LOB%', 'BABIP_x', 'BABIP_y']), X_test.drop(columns = ['LOB%', 'BABIP_x', 'BABIP_y']), y_train, 10, True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-SBIXPhT68-"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['LOB%', 'BABIP_x', 'BABIP_y'])\n",
    "X_test = X_test.drop(columns = ['LOB%', 'BABIP_x', 'BABIP_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next means of feature selection used was to use to train random forest and adaboost classifiers to access the feature importance attribute. ERA was the most important feature as was expected because most runs are earned runs, so it is nearly one to one with runs given up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iqr9erNAEt4E",
    "outputId": "662bddde-6302-4f74-c67b-a531981757cf"
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier() \n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = RFC.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7g_zXBOJ4sf",
    "outputId": "79f5b1d6-aa86-45c7-dcd6-829c636051bb"
   },
   "outputs": [],
   "source": [
    "ADA = AdaBoostClassifier() \n",
    "ADA.fit(X_train, y_train)\n",
    "\n",
    "feature_importances_ada = ADA.feature_importances_\n",
    "feature_importances_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "cYiu33pVEvvc",
    "outputId": "bbd54fe0-a4bc-47ab-cac7-30e9547da5c8"
   },
   "outputs": [],
   "source": [
    "sorted = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[sorted], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[sorted], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "3mHRZgBKKo1X",
    "outputId": "50f9f860-6999-4810-8256-4ffe1b8924d7"
   },
   "outputs": [],
   "source": [
    "sorted = np.argsort(feature_importances_ada)[::-1]\n",
    "\n",
    "plt.title('Feature Importances AdaBoost')\n",
    "plt.bar(range(X_train.shape[1]), feature_importances_ada[sorted], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[sorted], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCVmLW0RKKmQ"
   },
   "source": [
    "Here, I visualize each feature's correlation with every other feature, and then isolate each feauture's correlation with win percentage. Stats like WAR, FIP, and HR/9 stood out as statistics that could be best predictive of winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC22pLnWKKmQ"
   },
   "outputs": [],
   "source": [
    "corr_to_win = all_stats.corr()[['W']].sort_values('W', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "JPKZnYQzVEPY",
    "outputId": "d3814a4a-5e52-4d8c-84e6-e0aeb9891b81"
   },
   "outputs": [],
   "source": [
    "# Interesting to note how offensive statistics have higher correlations with one another than pitching statistics\n",
    "sns.heatmap(all_stats.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "Q5CPQm3tKKmQ",
    "outputId": "c14d744a-4a7b-4103-e8b7-5503d749781a"
   },
   "outputs": [],
   "source": [
    "corr_to_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmXsTlOXKKmS"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(corr_to_win)\n",
    "# make y-axis invisible in plot\n",
    "fig.update_yaxes(title = 'Correlation to W-L%', visible = True, showticklabels = True)\n",
    "fig.update_xaxes(title = 'Statistic', visible = True, showticklabels = True)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21CBLjIgY9-C"
   },
   "source": [
    "ERA is dominant in terms of AdaBoost and Random Forest feature importance, although it is not regarded as the best measurement of inidividual pitching performance because it is known to be dependent on the quality of the pitcher's defense. For the purposes of my investigation, I would like to be able to assume a pitcher's defense will be whatever the inputted lineup will offer them, so despite this stat's effectiveness in win prediction, my next step is to look into the tradeoff of using FIP or pitching WAR in its place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "az_WPAOPznfZ"
   },
   "source": [
    "## Deciding which pitching statistics to use with Logistic Regression as a baseline model\n",
    "FIP, ERA, and WAR_y have many common factors in their calculations, so I have decided to choose only one of the three so as to have diverse features that will not contradict each other in prediction\n",
    "- Note: Pitching WAR is calculated using the following formula: \n",
    "  \n",
    "  WAR = [[([(League “FIP” – “FIP”) / Pitcher Specific Runs Per Win] + Replacement Level) * (IP/9)] * Leverage Multiplier for Relievers] + League Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyrbYssw4gcf"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[['wRC+', 'HR/9', 'BsR', 'FIP', 'WAR_y', 'ERA', 'Def', 'SLG']]\n",
    "X_test = X_test[['wRC+', 'HR/9', 'BsR', 'FIP', 'WAR_y', 'ERA', 'Def', 'SLG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGzEmn56zP4S"
   },
   "outputs": [],
   "source": [
    "dicts = []\n",
    "pitch_stats = [{'ERA': ['FIP', 'WAR_y']}, {'WAR_y': ['FIP', 'ERA']}, {'FIP': ['ERA', 'WAR_y']}]\n",
    "for drop_stats in pitch_stats:\n",
    "    drop = list(drop_stats.values())\n",
    "    drop = drop[0]\n",
    "    stat = list(drop_stats.keys())\n",
    "    stat = stat[0]\n",
    "    xt = X_train.drop(columns = drop)\n",
    "    xte = X_test.drop(columns = drop)\n",
    "    LR = LogisticRegression()\n",
    "    LR = LR.fit(xt, y_train)\n",
    "    # Training predictions\n",
    "    y_pred_tr = LR.predict(xt)\n",
    "    # Testing predictions\n",
    "    y_pred_test = LR.predict(xte)\n",
    "    train_results = calc_metrics(y_pred_tr, y_train, 'Train ', False)\n",
    "    test_results = calc_metrics(y_pred_test, y_test, 'Test ', False)\n",
    "    train_results.update(test_results)\n",
    "    train_results['Feature Used'] = stat\n",
    "    dicts.append(train_results)\n",
    "df = pd.DataFrame(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "FEPS4qB_7hOT",
    "outputId": "c3a6c27f-b38b-4817-a0c1-931f55afa953"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "k_y5RQyo7DHe",
    "outputId": "7017a470-9801-4051-88e5-e973e87b96da"
   },
   "outputs": [],
   "source": [
    "px.bar(df, x = 'Feature Used', y = df.drop(columns = ['Feature Used', 'Test Error', 'Train Error']).columns, title=\"Logistic Regression Accuracy with Different Pitching Stats\",\n",
    "            labels={ # replaces default labels by column name,\n",
    "                'value': \"Accuracy\", 'variable': 'Accuracy Type'}, barmode = 'group', range_y = [.77, .92])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbbB2CgZIlBM"
   },
   "source": [
    "Despite the ERA model's better performance across the board, I will use the WAR model because it is a better representation of a single pitcher's performance rather than the defense and pitching combined of a team. FIP is part of the calculation of WAR, so the WAR model is the obvious choice considering it is a better indicator of team success (which is expected as it is normalized for competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOfjo_O7bLi8"
   },
   "source": [
    "# Finalizing Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNXyzh-tLZ2y"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[['wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]\n",
    "X_test = X_test[['wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]\n",
    "X_tr = X_tr[['wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]\n",
    "X_te = X_te[['wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]\n",
    "X = X[['wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRsuQpGtKKmT"
   },
   "source": [
    "<a id='visualization_2'></a>\n",
    "\n",
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "db = MySQLdb.connect(host='127.0.0.1', user='root', passwd='', db='mlb_db')\n",
    "tblchk = db.cursor()\n",
    "sql_team_data = pd.read_sql('SELECT * FROM team_data', con = db)\n",
    "team_data = clean_team_data(sql_team_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "j-wJZvxKKKmT",
    "outputId": "c4636290-0a46-4026-982b-491c46255d9b"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.scatter(team_data, x='BsR', y='W', hover_data = ['Team'], color = 'Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "7562jJE4KKmT",
    "outputId": "468dfaae-4817-4e08-dae7-ba67f1f391bb"
   },
   "outputs": [],
   "source": [
    "px.scatter(team_data, 'wRC_plus', 'W', hover_data = ['Team'], color = 'Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "I3Hy4wwOKKmU",
    "outputId": "4e833546-2929-4e44-d3d5-a7c713972561"
   },
   "outputs": [],
   "source": [
    "px.scatter(team_data, 'FIP', 'W', hover_data = ['Team'], color = 'Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "vuIJzJ6RKKmU",
    "outputId": "2d68c3d1-da5f-4180-f3aa-a21744e5af87"
   },
   "outputs": [],
   "source": [
    "px.scatter(team_data, 'WAR_y', 'W', hover_data = ['Team'], color = 'Season', log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "bzIdxhqHKKmU",
    "outputId": "0a4b9fb3-c713-485e-f5b7-1959d773e13c"
   },
   "outputs": [],
   "source": [
    "px.scatter(team_data, 'Def', 'W', hover_data = ['Team'], color = 'Season', log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDtDEUA-JysX"
   },
   "source": [
    "# Tuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the models, I build a function that will graph model performance (train and test accuracy) for different hyperparameter values, and I will run GridSearch before deciding which values will perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af87u7RkOJ1J"
   },
   "source": [
    "## Graphing hyperparameter tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwkfEEUjNz3w"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_hyper(model, val_list, val_name, arguments):\n",
    "    '''\n",
    "    Args:\n",
    "      model: Sklearn object with global variable representing hyperparameter value\n",
    "      val_list: values for hyperparameter to choose between\n",
    "      val_name: hyperparameter name for naming axes, titling\n",
    "\n",
    "    Purpose:\n",
    "      Graph the testing vs training accuracy for different values of hyperparameters\n",
    "    '''\n",
    "\n",
    "    values = {}\n",
    "    metrics_output = []\n",
    "\n",
    "    for threshold in val_list:\n",
    "        arguments[val_name] = threshold\n",
    "        clf = model(**arguments)\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        y_train_prediction = clf.predict(X_train)\n",
    "        y_test_prediction = clf.predict(X_test)\n",
    "        training_accuracy = accuracy_score(y_train, y_train_prediction)        \n",
    "        testing_accuracy = accuracy_score(y_test, y_test_prediction)\n",
    "\n",
    "        values = {val_name : threshold,\n",
    "                  'Training Accuracy' : training_accuracy,\n",
    "                  'Testing Accuracy' : testing_accuracy}\n",
    "        metrics_output.append(values)\n",
    "\n",
    "    return px.line(metrics_output, x= val_name, y = ['Training Accuracy', 'Testing Accuracy'], title=f'Accuracy for Various {val_name} Values', \n",
    "                 labels={'variable': 'Accuracy Type', 'value': 'Accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCfATNZoCoxA"
   },
   "source": [
    "## Tuning RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "YKQBpKxibtY_",
    "outputId": "84ab4783-b08a-4b78-bbb5-521952cd2bea"
   },
   "outputs": [],
   "source": [
    "graph_hyper(RandomForestClassifier, [10, 20, 30, 40, 50], 'n_estimators', {'n_estimators': 0, 'criterion': 'entropy', 'random_state': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P1ObjrdMXws"
   },
   "source": [
    "From this plot, it is clear there is not much accuracy to be gained from having more than 30 estimators:\n",
    "\n",
    "- Testing Accuracy goes from .849 to .851 when using 30 and 40 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "eiewoal9buHC",
    "outputId": "eddf4c75-b76b-476e-f6be-c5d75d11e8ba"
   },
   "outputs": [],
   "source": [
    "graph_hyper(RandomForestClassifier, range(1, 25, 1), 'max_depth', {'n_estimators': 30, 'max_depth': 0, 'criterion': 'entropy', 'random_state': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3M-duLxMvlk"
   },
   "source": [
    "From this plot, it is clear there is not much accuracy to be gained from having more than 6 as the max depth and the model clearly overfits after this threshold:\n",
    "\n",
    "- Testing Accuracy goes from .851 to .861 when using 6 and 10 as the max depth and the gap between training and testing accuracy balloons from .0018 to .0252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhhZ9aiQRAO6"
   },
   "source": [
    "### Using GridSearch to inform best hyperparameters based on graph output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvHYXDokQnii",
    "outputId": "c87acf07-c238-4cf9-9057-194b68bc7b35"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc = RandomForestClassifier()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [20, 30, 40]\n",
    "grid['max_depth'] = [6, 7, 8]\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHg8WylsRgFk"
   },
   "source": [
    "To avoid overfitting, I will use a max depth of 6 and 30 for n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzCFJA_bPKRd"
   },
   "source": [
    "## Tuning AdaBoost hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "qF5-M50tdBC7",
    "outputId": "5fc74468-9de1-49ef-d3d1-60c7831f51bd"
   },
   "outputs": [],
   "source": [
    "graph_hyper(AdaBoostClassifier, [10, 20, 30, 50, 60, 70, 80, 90, 100], 'n_estimators', {'n_estimators': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6k-_KY1QUV9"
   },
   "source": [
    "Testing accuracy actually exceeds training accuracy until 60 estimators were used. 80 estimators had practically the same training as testing accuracy and .004 better testing accuracy than the model with 50 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "C8M21om5dBxF",
    "outputId": "484b9303-6f2c-48a9-a836-3e2db5b2fcda"
   },
   "outputs": [],
   "source": [
    "graph_hyper(AdaBoostClassifier, [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 1.0], 'learning_rate', {'n_estimators': 50, 'learning_rate': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDTn5zTGRQYM"
   },
   "source": [
    "Testing accuracy actually exceeds training accuracy for the model with a learning rate of 1 and it has the highest accuracy of any model, so a learning rate of 1 is the obvious choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzzddR9tRMG4"
   },
   "source": [
    "### Using GridSearch to inform best hyperparameters based on graph output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNflRQ4AQMwT",
    "outputId": "a3d003b1-fe9d-4df4-fe58-b8a8b0aaecff"
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [60, 70, 80, 100]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=ada, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i1rOOIMRw1H"
   },
   "source": [
    "I will use a learning rate of 1 and 80 for n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG0Bi1uJy794"
   },
   "source": [
    "## MLP Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLa_iCOT0xai",
    "outputId": "fe81e0e7-42aa-42ea-f8e1-e1f47b0f6b6a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']}\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=grid, n_jobs=-1, cv=3, scoring='accuracy', verbose = 10)\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqydXqtN0lGv"
   },
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSdPSA3C9Hs9"
   },
   "source": [
    "## Gradient Booster Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92pjkm9Qa5Od",
    "outputId": "685fed6a-eceb-43c3-dc6c-31f3f2924178"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5zBDkr3gOju"
   },
   "source": [
    "## Comparing tuned model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With hyperparameter values set, this section collects metrics for each model using the final testing set to determine the best performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhjiKfuk_y3C"
   },
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_actual, test_or_train, print_bool):\n",
    "    '''\n",
    "    input empty string for test_or_train if not relevant\n",
    "    otherwise put train or test with trailing space\n",
    "    '''\n",
    "    a = accuracy_score(y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred)\n",
    "    prec = precision_score(y_actual, y_pred)\n",
    "    rec = recall_score(y_actual, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_actual, y_pred)\n",
    "\n",
    "    if(print_bool):\n",
    "        print(f'{test_or_train}Error: {1 - a}')\n",
    "        print(f'{test_or_train}Accuracy: {a}')\n",
    "        print(f'{test_or_train}Precision: {prec}')\n",
    "        print(f'{test_or_train}Recall: {rec}')\n",
    "        print(f'{test_or_train}F1: {f1}')\n",
    "        print(f'{test_or_train}AUC: {auc}')\n",
    "    return {f'{test_or_train}Accuracy': a, f'{test_or_train}Error': 1-a, f'{test_or_train}Precision': prec, f'{test_or_train}Recall': rec,\n",
    "          f'{test_or_train}F1 Score': f1, f'{test_or_train}AUC': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHnlY9lF4n-8"
   },
   "outputs": [],
   "source": [
    "for model in [GaussianNB(), LogisticRegression(), RandomForestClassifier(n_estimators = 30, max_depth = 6), AdaBoostClassifier(learning_rate = 1, n_estimators = 80),\n",
    "              MLPClassifier({'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'adam'})]:\n",
    "    print(model)\n",
    "    model = model.fit(X_tr, y_tr)\n",
    "    print('here')\n",
    "    y_pred = model.predict(X_te)\n",
    "    print_metrics(y_pred, y_te, '', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05qpQpwKqRfL"
   },
   "outputs": [],
   "source": [
    "final_results = list()\n",
    "for model in [GaussianNB(), LogisticRegression(), RandomForestClassifier(n_estimators = 30, max_depth = 8), AdaBoostClassifier(learning_rate = .3, n_estimators = 30),\n",
    "              MLPClassifier(activation= 'relu', alpha=0.05, hidden_layer_sizes= (100,), learning_rate= 'constant', solver= 'adam')]:\n",
    "    model.fit(X, y)\n",
    "    #final y predictions\n",
    "    y_pred_fin = model.predict(X_test)\n",
    "    #final y train predictions\n",
    "    y_pred_fin_tr = model.predict(X)\n",
    "    #Train results\n",
    "    tr = print_metrics(y_pred_fin_tr, y, 'Train ', False)\n",
    "    #Test results\n",
    "    te = print_metrics(y_pred_fin, y_test, 'Test ', False)\n",
    "    te.update(tr)\n",
    "    final_results.append(te)\n",
    "final = pd.DataFrame(final_results)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0v-vX2qussqD",
    "outputId": "9c1f362a-0035-4328-84d7-ea69f2778519"
   },
   "outputs": [],
   "source": [
    "first_column = ['Naiver Bayes', 'Logistic Regression', 'Random Forest Classifier', 'AdaBoostClassifier', 'MLP Classifier']\n",
    "\n",
    "final.insert(0, 'Model', first_column)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "qlUlcOeTFYle",
    "outputId": "18c3226f-19d5-4881-8145-0b4c56ebfce7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.bar(final, x = 'Model', y = ['Train Accuracy', 'Test Accuracy'], barmode = 'group', title='Accuracy for each Model', \n",
    "                 labels={'variable': 'Accuracy Type', 'value': 'Accuracy'}, range_y = [.72, .76])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model performs very similarly in terms of test accuracy. The MLP Classifier does achieve the best test accuracy and F1 score, indicating that it was the best performing model by a small margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6dnlwOUKKmV"
   },
   "source": [
    "# Scraping Player Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes code that was originally used in scraping data. I now have a seperate script entitled Automated_Data_Collection that refreshes the data in a local MySQL database. The scraping process is time consuming, so I took this step to allow for a better user experience in the app I have created using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qz2NsJV8KKmV",
    "outputId": "97c2f968-17dd-4008-f3e8-8276e81a002c"
   },
   "outputs": [],
   "source": [
    "# beginning of sample is 2015\n",
    "year = 2021\n",
    "wrc = pd.DataFrame()\n",
    "pitch = pd.DataFrame()\n",
    "field = pd.DataFrame()\n",
    "# sustainable way of changing year without change in code\n",
    "while year < datetime.datetime.now().year + 1:\n",
    "    for num in range(int(pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate=&enddate=&page=1_50')[16].columns[0][0][-8:-6].strip())):\n",
    "        # scrape hitting data\n",
    "        if (num < 1):\n",
    "            temp = pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate=&enddate=&page={str(num + 1)}_50')[16][:-1]   \n",
    "            temp.columns = temp.columns.droplevel(0)\n",
    "            wrc_df = temp\n",
    "        else:\n",
    "            temp = (pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate=&enddate=&page={str(num + 1)}_50')[16][:-1])\n",
    "            temp.columns = temp.columns.droplevel(0)\n",
    "            wrc_df = wrc_df.append(temp)\n",
    "        # getting rid of the final row with non-numeric data above\n",
    "    wrc_df['Season'] = year\n",
    "    wrc = wrc.append(wrc_df)\n",
    "    # scrape pitching data\n",
    "    for num in range(int(pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate={year}-01-01&enddate={year}-12-31&sort=21,d&page=1_50')[16].columns[0][0][-8:-6].strip())):\n",
    "        # scrape hitting data\n",
    "        if (num < 1):\n",
    "            temp = pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate={year}-01-01&enddate={year}-12-31&sort=21,d&page={str(num + 1)}_50')[16][:-1]   \n",
    "            temp.columns = temp.columns.droplevel(0)\n",
    "            pitch_df = temp\n",
    "        else:\n",
    "            temp = (pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate={year}-01-01&enddate={year}-12-31&sort=21,d&page={str(num + 1)}_50')[16][:-1])\n",
    "            temp.columns = temp.columns.droplevel(0)\n",
    "            pitch_df = pitch_df.append(temp)\n",
    "\n",
    "        # getting rid of the final row with non-numeric data above\n",
    "    pitch_df['Season'] = year\n",
    "    pitch = pitch.append(pitch_df)\n",
    "    print(year)\n",
    "    year+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGbApoZYhVnH"
   },
   "outputs": [],
   "source": [
    "def clean_player_data(hit_df, pitch_df):\n",
    "    '''\n",
    "    function intended to make statistics numerical, manually calculate statistics, and set the indices to Name and Season\n",
    "\n",
    "    Args:\n",
    "    wrc (pd.DataFrame) contains individual player data by season\n",
    "    pitch (pd.DataFrame) contains individual pitcher data by season\n",
    "\n",
    "    Returns wrc, pitch as clean datasets for use in App'''\n",
    "\n",
    "    # applying the function to each column to ensure all data points are numerical\n",
    "    for col in hit_df.columns:\n",
    "        if col not in ['Name', 'Team', 'Season', 'GB', 'Pos']:\n",
    "            hit_df[col] = hit_df[col].apply(string_to_num)\n",
    "    for col in pitch_df.columns:\n",
    "        if col not in ['Name', 'Team', 'Season', 'GB']:\n",
    "            pitch_df[col] = pitch_df[col].apply(string_to_num)\n",
    "    #Determining home runs allowed for each player for easier calculation\n",
    "    pitch_df['HR'] = pitch_df['HR/9'] * pitch_df['IP'] * 9\n",
    "    #Determining total bases for each player for more accurate slugging percentage calculation\n",
    "    # First must find at bats by subtracting walks using walk percentage\n",
    "    # Calculation ignores HBP\n",
    "    hit_df['AB'] = hit_df['PA'] * (1 - (hit_df['BB%'] * .01))\n",
    "    # Calculation necessary for determining slugging percentage over multiple seasons\n",
    "    hit_df['TB'] = hit_df['SLG'] * hit_df['AB']\n",
    "    pitch_df.set_index(['Name', 'Season'], inplace = True)\n",
    "    hit_df.set_index(['Name', 'Season'], inplace = True)\n",
    "    return hit_df, pitch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dem1HxgrjOmj"
   },
   "outputs": [],
   "source": [
    "hit_df, pitch_df = clean_player_data(wrc, pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJhKK7K0cFVG"
   },
   "source": [
    "# Aggregating statistics and generating predictions for customized lineups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jS7ob5F3Yqv"
   },
   "source": [
    "Here, I create class objects for pitchers in hitters, so that information about different types of players can be processed more easily and in a more organized manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYpK4dyav_pG"
   },
   "outputs": [],
   "source": [
    "class pitcher:\n",
    "    hr = 0\n",
    "    war = 0\n",
    "    # parameterized constructor\n",
    "    def __init__(self, name, seasons, ip):\n",
    "        pitch_df = ui_pitch_df.set_index(['Name', 'Season'])\n",
    "        pitcher = pitch_df.loc[name]\n",
    "        #eliminating seasons that the player did not play in the given range\n",
    "        played = list()\n",
    "        for seas in seasons:\n",
    "            if seas in pitcher.index:\n",
    "                played.append(seas)\n",
    "        pitcher = pitcher.loc[played]\n",
    "        #finds pitcher HR allowed per inning pitched\n",
    "        self.hr = pitcher['HR'].sum() / pitcher['IP'].sum()\n",
    "        #finds pitcher WAR per inning pitched\n",
    "        self.war = pitcher['WAR'].sum() / pitcher['IP'].sum()\n",
    "        #Number of innings to be pitched by player (hypothetically)\n",
    "        self.ip = ip\n",
    "        self.name = name\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"HR/9: \" + str(self.hr / 9))\n",
    "        print(\"WAR/9 \" + str(self.war / 9))\n",
    "\n",
    "    def setIP(self, ip):\n",
    "        self.ip = ip\n",
    "\n",
    "    def scale(self, innings):\n",
    "        self.hr = self.hr * innings\n",
    "        self.war = self.war * innings\n",
    "\n",
    "    def getHR(self):\n",
    "        return self.hr\n",
    "\n",
    "    def getWAR(self):\n",
    "        return self.war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avITmP_A3yUz"
   },
   "outputs": [],
   "source": [
    "class hitter:\n",
    "    wrcp = 0\n",
    "    bsr = 0\n",
    "    defn = 0\n",
    "    slg = 0\n",
    "    # parameterized constructor\n",
    "    def __init__(self, name, seasons, games):\n",
    "        hit_df = ui_hit_df.set_index(['Name', 'Season'])\n",
    "        hitter = hit_df.loc[name]\n",
    "        #eliminating seasons that the player did not play in the given range\n",
    "        played = list()\n",
    "        for seas in seasons:\n",
    "            if seas in hitter.index:\n",
    "                played.append(seas)\n",
    "        hitter = hitter.loc[played]\n",
    "        #wRC+ is normalized season by season, so average is taken across inputted season range; drawback is smaller sample sizes may have greater effect than hoped\n",
    "        #Users will be able to see season by season stats for the player, so they can use their own intuition to evaluate validity of using a given season for a player\n",
    "        self.wrcp = hitter['wRC+'].mean()\n",
    "        #finds slugging percentage\n",
    "        self.slg = hitter['TB'].sum() / hitter['AB'].sum()\n",
    "        #finds defense per game\n",
    "        self.defn = hitter['Def'].sum() / hitter['G'].sum()\n",
    "        #finds baserunning per game\n",
    "        self.bsr = hitter['BsR'].sum() / hitter['G'].sum()\n",
    "        #Number of games to be played by player (hypothetically)\n",
    "        self.games = games\n",
    "        self.name = name\n",
    "    \n",
    "    def display(self):\n",
    "        # displays statistics at a 162 game pace\n",
    "        print(\"wRC+: \" + str(self.wrcp))\n",
    "        print(\"BsR: \" + str(self.bsr * 162))\n",
    "        print(\"Def: \" + str(self.defn * 162))\n",
    "        print(\"SLG: \" + str(self.slg))\n",
    "\n",
    "    def setGames(self, games):\n",
    "        self.games = games\n",
    "\n",
    "    def scale(self, games):\n",
    "        self.wrcp = self.wrcp * games\n",
    "        self.bsr = self.bsr * games\n",
    "        self.defn = self.defn * games\n",
    "        self.slg = self.slg * games\n",
    "\n",
    "    def getWRC(self):\n",
    "        return self.wrcp\n",
    "\n",
    "    def getBsR(self):\n",
    "        return self.bsr\n",
    "\n",
    "    def getDef(self):\n",
    "        return self.defn\n",
    "\n",
    "    def getSLG(self):\n",
    "        return self.slg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMSXJV4EKKmW"
   },
   "outputs": [],
   "source": [
    "def pitcher_df(rotation):\n",
    "    #Accumulating dictionaries representing players into a list\n",
    "    ps = list()\n",
    "    for p in rotation:\n",
    "        row = {'Name': p.name, 'IP': p.ip, 'HR': p.hr, 'WAR': p.war}\n",
    "        ps.append(row)\n",
    "    return pd.DataFrame(ps)\n",
    "\n",
    "def hitter_df(lineup):\n",
    "    #Accumulating dictionaries representing players into a list\n",
    "    hs = list()\n",
    "    for batter in lineup:\n",
    "        row = {'Name': batter.name, 'G': batter.games, 'wRC+': batter.wrcp, 'BsR': batter.bsr, 'Def': batter.defn, 'SLG': batter.slg}\n",
    "        hs.append(row)\n",
    "    return pd.DataFrame(hs)\n",
    "\n",
    "def wins_for_team(lineup, rotation, model='standard'):\n",
    "    '''\n",
    "    lineup (list) consists of hitter objects\n",
    "    pitchers (list) consists of pitcher objects\n",
    "    model (Sklearn object) model trained on team data to be used to classify customized team (using predict_proba)\n",
    "    '''\n",
    "\n",
    "    #Scaling each players statistics to have their contribution correspond to their designated innings pitched or games played\n",
    "    for batter in lineup:\n",
    "        batter.display()\n",
    "        #multiplies each batting statistic by the games they play\n",
    "        batter.scale(batter.games)\n",
    "        batter.display()\n",
    "    for p in rotation:\n",
    "        p.display()\n",
    "        #multiplies each pitching statistic by the innings they pitch\n",
    "        p.scale(p.ip)\n",
    "        p.display()\n",
    "\n",
    "    #Creating dataframes for simpler aggregation\n",
    "    lineup = hitter_df(lineup)\n",
    "    rotation = pitcher_df(rotation)\n",
    "\n",
    "    #ensuring that there are 1458 games played by position players and innings thrown by pitchers\n",
    "    games = lineup['G'].sum()\n",
    "    if games != 1458:\n",
    "        raise Exception(f'Total Games inputted: {games}, must be 1458')\n",
    "    ip = rotation['IP'].sum()\n",
    "    if ip != 1458:\n",
    "        raise Exception(f'Total IP inputted: {ip}, must be 1458')\n",
    "    \n",
    "    stats = dict()\n",
    "    #when scaled wrc is multiplied by games played for each player to ensure proportionate contribution\n",
    "    stats['wRC+'] = lineup['wRC+'].sum() / 1458\n",
    "    #when scaled slg is multiplied by games played for each player to ensure proportionate contribution\n",
    "    stats['SLG'] = lineup['SLG'].sum() / 1458\n",
    "    #The equivalent of a single team's defense metric is the sum of their entire lineup's Def (which is why the denominator is 162)\n",
    "    stats['Def'] = lineup['Def'].sum() / 162\n",
    "    stats['BsR'] = lineup['BsR'].sum() / 162\n",
    "    stats['HR/9'] = rotation['HR'].sum() / (9 * 1458)\n",
    "    stats['WAR'] = rotation['WAR'].sum() / (162)\n",
    "    reg_stats = stats.copy()\n",
    "    #Stored normalization factors from team data\n",
    "    metrics = ['wRC+', 'HR/9', 'BsR', 'WAR', 'Def', 'SLG']\n",
    "    for stat in metrics:\n",
    "        stats[stat] = (stats[stat] - scales.loc[stat]['Mean']) / scales.loc[stat]['Unit Variance']\n",
    "    if (type(model) == str):\n",
    "        logReg = LogisticRegression().fit(X, y)\n",
    "        adaBoost = AdaBoostClassifier(learning_rate = .3, n_estimators = 30).fit(X, y)\n",
    "        wins = logReg.predict_proba([[stats['wRC+'], stats['HR/9'], stats['BsR'], stats['WAR'], stats['Def'], stats['SLG']]])[0][1] * 2\n",
    "        wins += adaBoost.predict_proba([[stats['wRC+'], stats['HR/9'], stats['BsR'], stats['WAR'], stats['Def'], stats['SLG']]])[0][1]\n",
    "        wins /= 3\n",
    "        wins *= 162\n",
    "        return wins, reg_stats\n",
    "    else:\n",
    "        return model.predict_proba([[stats['wRC+'], stats['HR/9'], stats['BsR'], stats['WAR'], stats['Def'], stats['SLG']]])[0][1] * 162, reg_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUIW9-qioEgO"
   },
   "source": [
    "Each model performs very similarly, but has drastically different predictions; the next step is to use actual seasons for teams and choose the algorithm that minimizes the difference between predicted and actual win total.\n",
    "\n",
    "With each model being relatively the same in terms of accuracy, I am looking find the best combination of algorithms in mimicking season long win totals, historically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11wJoCATbjSJ"
   },
   "source": [
    "# Collecting team data to compare model predictions to actual full season win totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89PrPt2MKKmJ",
    "outputId": "5e3ac841-93c1-437c-d2ee-0bfdce8fb878"
   },
   "outputs": [],
   "source": [
    "def collect_team_data_yearly(year):\n",
    "\n",
    "  '''\n",
    "  Args:\n",
    "    year (integer): year to start collecting data from\n",
    "  Collecting team data to use as testing data\n",
    "  '''\n",
    "  \n",
    "  wrc = pd.DataFrame()\n",
    "  pitch = pd.DataFrame()\n",
    "  field = pd.DataFrame()\n",
    "  # sustainable way of changing year without change in code\n",
    "  while year < datetime.datetime.now().year + 1:\n",
    "      # scrape hitting data\n",
    "      wrc_df = pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0,ts&rost=0&age=0&filter=&players=0&startdate=&enddate=')\n",
    "      # getting rid of the final row with non-numeric data\n",
    "      wrc_df = wrc_df[16][:-1]\n",
    "      wrc_df[('temp', 'Season')] = year\n",
    "      wrc_df.columns = wrc_df.columns.droplevel(0)\n",
    "      wrc = pd.concat([wrc, wrc_df], axis = 0)\n",
    "      # scrape pitching data\n",
    "      pitch_df = pd.read_html(f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=8&season={year}&month=0&season1={year}&ind=0&team=0,ts&rost=0&age=0&filter=&players=0&startdate=&enddate=')\n",
    "      # getting rid of the final row with non-numeric data\n",
    "      pitch_df = pitch_df[16][:-1]\n",
    "      pitch_df[('temp', 'Season')] = year\n",
    "      pitch_df.columns = pitch_df.columns.droplevel(0)\n",
    "      pitch = pd.concat([pitch, pitch_df], axis = 0)\n",
    "      year += 1\n",
    "  return wrc, pitch\n",
    "\n",
    "w, p = collect_team_data_yearly(1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkkJ9nJj8o-V"
   },
   "outputs": [],
   "source": [
    "team_data = pd.merge(w, p, left_on = ['Season', 'Team'], right_on = ['Season', 'Team'], how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUh9XQoBqdEo"
   },
   "source": [
    "Changing each column to be numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB9CBBAPqdEo"
   },
   "outputs": [],
   "source": [
    "# applying the function to each column to ensure all data points are numerical\n",
    "for col in team_data.columns:\n",
    "    if col not in ['Team', 'Season', 'GB']:\n",
    "        team_data[col] = team_data[col].apply(string_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GtE8XYu9A-t"
   },
   "outputs": [],
   "source": [
    "team_data['W'] = team_data['W'] * (162 / team_data['GS'])\n",
    "# Saving a copy of the scraped data \n",
    "saved_team_data = team_data.copy()\n",
    "team_data = saved_team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZN6EEtBsBo9"
   },
   "outputs": [],
   "source": [
    "team_data = team_data.rename(columns = {'Def_x': 'Def'})\n",
    "wins = team_data['W'].reset_index(drop=True)\n",
    "team_data = team_data[['Team', 'Season', 'wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]\n",
    "team_data\n",
    "team_data['BsR'] = team_data['BsR'] / 162\n",
    "team_data['WAR_y'] = team_data['WAR_y'] / 162\n",
    "team_data['Def'] = team_data['Def'] / 162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDQ5DIQgwAU1"
   },
   "outputs": [],
   "source": [
    "#Renaming for agreement with scales dataframe\n",
    "team_data.rename(columns = {'WAR_y': 'WAR'}, inplace = True)\n",
    "metrics = ['wRC+', 'HR/9', 'BsR', 'WAR', 'Def', 'SLG']\n",
    "for stat in metrics:\n",
    "  team_data[stat] = (team_data[stat] - scales.loc[stat]['Mean']) / scales.loc[stat]['Unit Variance']\n",
    "#Changing WAR back for agreement with model\n",
    "team_data.rename(columns = {'WAR': 'WAR_y'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "dxjEKkkDxFvv",
    "outputId": "db4be177-e337-48fd-cdda-b371552abfd8"
   },
   "outputs": [],
   "source": [
    "team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "T617fRIoulse",
    "outputId": "d894d642-a6f5-4552-fb86-b49a49fe7d0e"
   },
   "outputs": [],
   "source": [
    "win_preds = pd.DataFrame()\n",
    "names = ['Naive Bayes', 'Log Reg', 'Random Forest', 'AdaBoost', 'MLP Classifier', 'GBoost']\n",
    "idx = 0\n",
    "for model in [GaussianNB(), LogisticRegression(), RandomForestClassifier(n_estimators = 30, max_depth = 8), AdaBoostClassifier(learning_rate = .3, n_estimators = 30),\n",
    "              MLPClassifier(activation= 'relu', alpha=0.05, hidden_layer_sizes= (100,), learning_rate= 'constant', solver= 'adam'), GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0)]:\n",
    "    model.fit(X, y)\n",
    "    pred = model.predict_proba(team_data.drop(columns=['Team', 'Season'])) * 162\n",
    "    pred = pred[:, 1]\n",
    "    win_preds[names[idx]] = pred\n",
    "    idx+=1\n",
    "team_data = team_data.reset_index(drop=True)\n",
    "final = pd.concat([team_data, win_preds], axis = 1)\n",
    "final['Actual Wins'] = wins\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEivSK91tpGc"
   },
   "source": [
    "Logistic Regression is the strongest candidate to use for win prediction because of how it minimizes error with reasonable variance. AdaBoost has very minimal variance in its predictions, which might help to offset some of the error at the extremes of win totals (upon statistical investigation, teams with very high win totals tend to be predicted to perform even better and teams with low win totals tend to be predicted to perform even worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmCU4HmhoLTW"
   },
   "outputs": [],
   "source": [
    "final['LGAD5050'] = (final['Log Reg'] + final['AdaBoost']) / 2\n",
    "final['LGAD21'] = (2*final['Log Reg'] + final['AdaBoost']) / 3\n",
    "final['LGAD32'] = (3*final['Log Reg'] + 2*final['AdaBoost']) / 5\n",
    "final['LGAD12'] = (final['Log Reg'] + 2*final['AdaBoost']) / 3\n",
    "final['LGAD23'] = (2*final['Log Reg'] + 3*final['AdaBoost']) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhqytQAxqrMy"
   },
   "outputs": [],
   "source": [
    "for col in ['Naive Bayes', 'Log Reg', 'Random Forest', 'AdaBoost', 'MLP Classifier', 'GBoost', 'LGAD5050', 'LGAD21', 'LGAD32', 'LGAD12', 'LGAD23']:\n",
    "    new_name = col + ' Diff'\n",
    "    final[new_name] = final[col] - final['Actual Wins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XC8_90HV4jwN"
   },
   "outputs": [],
   "source": [
    "for col in ['Naive Bayes', 'Log Reg', 'Random Forest', 'AdaBoost', 'MLP Classifier', 'GBoost', 'LGAD5050', 'LGAD21', 'LGAD32', 'LGAD12', 'LGAD23']:\n",
    "    new_name = col + ' Absolute Diff'\n",
    "    final[new_name] = abs(final[col] - final['Actual Wins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "7dvfokmn5Ef3",
    "outputId": "3c8c2eb7-0b26-45aa-800a-8e6737e8e518"
   },
   "outputs": [],
   "source": [
    "final.sort_values(by = 'Actual Wins', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GkVRXuVBqjm"
   },
   "source": [
    "I am checking to see which periods in history are too different to be included in the analysis\n",
    "\n",
    "- HR/9, Defense, and SLG are all too different in the years before 1962 to rationalize using these years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pncCUFDLAj6X",
    "outputId": "bc410000-2809-448a-cc87-11e8bb77aac9"
   },
   "outputs": [],
   "source": [
    "early = final.loc[final.Season < 1962]\n",
    "middle = final.loc[(final.Season > 1962) & (final.Season < 2000)]\n",
    "recent = final.loc[(final.Season > 2000)]\n",
    "pd.DataFrame({'pre-1962': early.mean(), 'pre-2000, post-1962': middle.mean(), 'post-2000': recent.mean()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjrH57XEDyQf"
   },
   "source": [
    "Excluding seasons before 1962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9gRmKgkCFzi"
   },
   "outputs": [],
   "source": [
    "final = final.loc[final.Season > 1962]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfy88nF5GCng"
   },
   "source": [
    "Revisit and ensure statistics are aggregated correclty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92PXWJXUSkg"
   },
   "source": [
    "Visualizing the difference between actual win totals and the predictions of each model\n",
    "\n",
    "- This graph will be used to determine which model's predict_proba function translates best to an entire season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "NOwZXHQdUmUK",
    "outputId": "b23b783d-34c2-454d-ff37-304568fe56a3"
   },
   "outputs": [],
   "source": [
    "px.scatter(final, x='Actual Wins', y = ['Naive Bayes Diff', 'Log Reg Diff', 'Random Forest Diff', 'AdaBoost Diff', 'MLP Classifier Diff', 'GBoost Diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "gr51XVqxqV5I",
    "outputId": "b2019864-2186-404a-d828-b11e0f57db9a"
   },
   "outputs": [],
   "source": [
    "px.bar(pd.DataFrame(final.mean()).loc[['Naive Bayes Absolute Diff', 'Log Reg Absolute Diff',\n",
    "       'Random Forest Absolute Diff', 'AdaBoost Absolute Diff',\n",
    "       'MLP Classifier Absolute Diff', 'GBoost Absolute Diff']], title = 'Average Absolute Error in Win Prediction', labels = {'index': 'model', 'value': 'Error (in wins)'}).update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "aVxDl6EwWLl2",
    "outputId": "8f766be3-d6fc-4f6b-822c-d6dfb2a3d995"
   },
   "outputs": [],
   "source": [
    "px.scatter(final, x='Actual Wins', y = ['Naive Bayes Diff', 'Log Reg Diff', 'MLP Classifier Diff'], hover_data=('Team', 'Season', 'wRC+', 'WAR_y'), labels={'value': 'Error (in wins)', 'variable': 'Algorithm'},\n",
    "           trendline = 'ols', trendline_scope = 'trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "sxbDEOlZHI4l",
    "outputId": "b5f8b68c-14d8-4796-aa04-c188a409737e"
   },
   "outputs": [],
   "source": [
    "px.bar(pd.DataFrame(final.mean()).loc[['Naive Bayes Diff', 'Log Reg Diff',\n",
    "       'Random Forest Diff', 'AdaBoost Diff', 'MLP Classifier Diff',\n",
    "       'GBoost Diff']], title = 'Average Error in Win Prediction', labels = {'index': 'model', 'value': 'Error (in wins)'}).update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "yRwBmqLXXpiq",
    "outputId": "9bdb3f61-df66-4d0b-d0ce-a6a14b001a46"
   },
   "outputs": [],
   "source": [
    "px.histogram(final, x='Actual Wins', y = ['LGAD5050 Diff', 'LGAD21 Diff', 'LGAD32 Diff'], barmode = 'group', hover_data=('Team', 'Season', 'wRC+', 'WAR_y'), labels={'value': 'Error (in wins)', 'variable': 'Algorithm'}, histfunc = 'avg', nbins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "PMKwAySWWxQx",
    "outputId": "484bdb27-99b8-4cb4-9d55-36401a7b7380"
   },
   "outputs": [],
   "source": [
    "px.scatter(final, x='Actual Wins', y = ['LGAD21 Diff'], hover_data=('Team', 'Season', 'wRC+', 'WAR_y'), labels={'value': 'Error (in wins)', 'variable': 'Algorithm'},\n",
    "           trendline = 'ols', trendline_scope = 'trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyRT_kIMRndo"
   },
   "outputs": [],
   "source": [
    "fin = final.loc[final['Actual Wins'] > 81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q0iFZwqGEFb"
   },
   "source": [
    "# Final model selection\n",
    "\n",
    "- Using two parts logistic regression and one part AdaBoostClassifier yields the smallest error when considering the last 60 years. These algorithms balance each other out - the AdaBoostClassifier is more certain in its predictions (predict_proba values closer to 0 and 1), while the LogisticRegression's predictions are not as absolute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMvbhUXngBzQ"
   },
   "source": [
    "need table to view player statistics with sorting capabilities\n",
    "- default state contains all seasons\n",
    "- seasons picker will filter out seasons, groupby player - (single season check box to allow no aggregation)\n",
    "  - Submit button so that it does not update based on season until start and end are confirmed\n",
    "- team filter\n",
    "- player filter\n",
    "\n",
    "need graph to compare customized lineup to other teams historically\n",
    "- shows wins on one axis, chosen statistic (wrc default) on the other (may need to store customized team stats prior to normalization\n",
    "- ideally, hovering over team data point would show that team's stats in table broken down by player\n",
    "\n",
    "need drop down for players with games played slider defaulted to 162, submit button\n",
    "- Games to be allocated left text should automatically adjust\n",
    "- Start Season Dropdown, end season dropdown\n",
    "- Upon submission, player is added to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94KGzYlLiEm9"
   },
   "outputs": [],
   "source": [
    "saved_hit_df = hit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olEbD1gIjXx2"
   },
   "outputs": [],
   "source": [
    "saved_pitch_df = pitch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BicNNJqoQQ2"
   },
   "outputs": [],
   "source": [
    "ui_hit_df = hit_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUYzDbfeoQXJ"
   },
   "outputs": [],
   "source": [
    "ui_pitch_df = pitch_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPwIqCQSytPt"
   },
   "outputs": [],
   "source": [
    "#hover_data=('Team', 'Season', 'wRC+', 'WAR_y')\n",
    "team_history = saved_team_data[['Team', 'Season', 'wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG', 'W']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYt-ncX90749"
   },
   "source": [
    "## PLAN: isolate the dcc.store inputs to identify what is going so wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbgg94a8Cg6Y"
   },
   "source": [
    "***Chained Callbacks and dcc.Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "db = MySQLdb.connect(host='127.0.0.1', user='root', passwd='', db='mlb_db')\n",
    "tblchk = db.cursor()\n",
    "# The year of the latest record in the data table\n",
    "sql_game_data = pd.read_sql('SELECT * FROM game_data', con = db)\n",
    "sql_team_data = pd.read_sql('SELECT * FROM team_data', con = db)\n",
    "sql_hitter_data = pd.read_sql('SELECT * FROM hitter_data', con = db)\n",
    "sql_pitcher_data = pd.read_sql('SELECT * FROM pitcher_data', con = db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_team_data[sql_team_data['Season'] == '2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_col_mapping = {'BB%': 'BB_pct', 'K%': 'K_pct', 'wRC+': 'wRC_plus', 'K/9': 'K_per_9',\n",
    "       'BB/9': 'BB_per_9', 'HR/9': 'HR_per_9', 'LOB%': 'LOB_pct', 'GB%': 'GB_pct', 'HR/FB': 'HR_per_FB', 'vFA (pi)': 'vFA'}\n",
    "python_col_mapping = {v: k for k, v in sql_col_mapping.items()}\n",
    "sql_game_data.rename(columns = python_col_mapping, inplace = True)\n",
    "sql_team_data.rename(columns = python_col_mapping, inplace = True)\n",
    "sql_hitter_data.rename(columns = python_col_mapping, inplace = True)\n",
    "sql_pitcher_data.rename(columns = python_col_mapping, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_game_data(all_stats):\n",
    "    all_stats = all_stats[all_stats.GS == '1']\n",
    "    # These columns have only null values for single games\n",
    "    all_stats.drop(columns = ['xwOBA', 'xERA', 'vFA (pi)'], inplace = True)\n",
    "    # applying the function to each column to ensure all data points are numerical\n",
    "    for col in all_stats.columns:\n",
    "        if col not in ['Team', 'Date', 'GB']:\n",
    "            all_stats[col] = all_stats[col].apply(string_to_num)\n",
    "    all_stats = all_stats.drop(columns = ['Team', 'G', 'PA', 'R',\n",
    "           'Date', 'L', 'SV', 'GS', 'IP', 'RBI'])\n",
    "    # Only ~100 columns with null values\n",
    "    all_stats.dropna(inplace = True)\n",
    "    X = all_stats.drop(columns = ['W'])\n",
    "    X = X[['wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG']]\n",
    "    cols = X.columns\n",
    "    y = all_stats['W']\n",
    "    #Scaling each column to be \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X = pd.DataFrame(X, columns = cols)\n",
    "    #Storing values used to scale each feature for manual normalization in later step\n",
    "    feat_names = ['wRC+', 'HR/9', 'BsR', 'WAR', 'Def', 'SLG']\n",
    "    scales = pd.DataFrame({'Feature': feat_names, 'Unit Variance': scaler.scale_, 'Mean': scaler.mean_})\n",
    "    scales.set_index('Feature', inplace = True)\n",
    "    return X, y, scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify what is going wrong with win calculation in database, fix those records with invalid win totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_team_data(team_data):\n",
    "    # applying the function to each column to ensure all data points are numerical\n",
    "    for col in team_data.columns:\n",
    "        if col not in ['Team', 'Season', 'GB']:\n",
    "            team_data[col] = team_data[col].apply(string_to_num)\n",
    "    # Saving a copy of the scraped data \n",
    "    return team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_data(hit_df, pitch_df):\n",
    "    '''\n",
    "    function intended to make statistics numerical, manually calculate statistics, and set the indices to Name and Season\n",
    "\n",
    "    Args:\n",
    "    wrc (pd.DataFrame) contains individual player data by season\n",
    "    pitch (pd.DataFrame) contains individual pitcher data by season\n",
    "\n",
    "    Returns wrc, pitch as clean datasets for use in App'''\n",
    "    \n",
    "    hit_df = hit_df[hit_df['wRC+'] != None]\n",
    "    pitch_df.dropna(inplace=True)\n",
    "    # applying the function to each column to ensure all data points are numerical\n",
    "    for col in hit_df.columns:\n",
    "        if col not in ['Name', 'Team', 'GB', 'Pos']:\n",
    "            hit_df[col] = hit_df[col].apply(string_to_num)\n",
    "    for col in pitch_df.columns:\n",
    "        if col not in ['Name', 'Team', 'GB']:\n",
    "            pitch_df[col] = pitch_df[col].apply(string_to_num)\n",
    "    #Determining home runs allowed for each player for easier calculation\n",
    "    pitch_df['HR'] = pitch_df['HR/9'] * pitch_df['IP'] * 9\n",
    "    #Determining total bases for each player for more accurate slugging percentage calculation\n",
    "    # First must find at bats by subtracting walks using walk percentage\n",
    "    # Calculation ignores HBP\n",
    "    hit_df['AB'] = hit_df['PA'] * (1 - (hit_df['BB%'] * .01))\n",
    "    # Calculation necessary for determining slugging percentage over multiple seasons\n",
    "    hit_df['TB'] = hit_df['SLG'] * hit_df['AB']\n",
    "    return hit_df, pitch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clean_team_data(sql_team_data)\n",
    "a[a.Season == '2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y, scales = clean_game_data(sql_game_data)\n",
    "ui_hit_df, ui_pitch_df = clean_player_data(sql_hitter_data, sql_pitcher_data)\n",
    "team_history = clean_team_data(sql_team_data)\n",
    "team_history = team_history[['Team', 'Season', 'wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG', 'W']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_hit_df.reset_index(inplace=True)\n",
    "ui_pitch_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_hit_df = ui_hit_df.head()\n",
    "ui_pitch_df = ui_pitch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "BIaOJoiRKxtb",
    "outputId": "3625f808-3b01-4242-a8d8-5a97691917dd"
   },
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)\n",
    "\n",
    "def generate_table(dataframe, id):\n",
    "    return html.Table(\n",
    "        # Header\n",
    "        [html.Tr([html.Th(col) for col in dataframe.columns]) ] +\n",
    "        # Body\n",
    "        [html.Tr([\n",
    "            html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "        ]) for i in range(len(dataframe))], id = id\n",
    "    )\n",
    "#hitters selected\n",
    "hit_sel = pd.DataFrame(columns = ['Name', 'Years', 'Games'])\n",
    "#pitchers selected\n",
    "pit_sel = pd.DataFrame(columns = ['Name', 'Years', 'Innings'])\n",
    "#current year\n",
    "curr_year = datetime.datetime.now().year\n",
    "first_year = int(ui_hit_df['Season'].min())\n",
    "games = 1458\n",
    "innings = 1458\n",
    "\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    #Storing the hitters selected in a df\n",
    "    dcc.Store(id = 'sel_tbl', data = [], storage_type = 'memory'),\n",
    "    #Storing the remaining number of games\n",
    "    dcc.Store(id = 'games_rem', data = [], storage_type = 'memory'),\n",
    "    #Storing the pitchers selected in a df\n",
    "    dcc.Store(id = 'psel_tbl', data = [], storage_type = 'memory'),\n",
    "    #Storing the remaining number of innings\n",
    "    dcc.Store(id = 'inn_rem', data = [], storage_type = 'memory'), \n",
    "    #Storing team stats to be displayed in scatter plot  \n",
    "    dcc.Store(id = 'team_stats', data = [], storage_type = 'memory'),\n",
    "    html.Div(html.Label(\"MLB Build a Team\"), style = {'text-align': 'center', 'font-size': '25px', 'vertical-align': 'top',\n",
    "                                           'align': 'center', 'width': '100%', 'margin-top': '40px'}),              \n",
    "    #PLAYER SELECTION\n",
    "    #Multi DropDown for hitters\n",
    "    html.Div(children = [\n",
    "        \n",
    "        html.Label([\n",
    "            \"Hitter\",\n",
    "            dcc.Dropdown(\n",
    "                #Dropdown with players to be inputted into algo\n",
    "                id='hitter-dd-calc', clearable=True,\n",
    "                multi = False,\n",
    "                value=[], options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in ui_hit_df['Name'].unique()\n",
    "                ])\n",
    "        ]),\n",
    "        #Start year\n",
    "        html.Label(['Start Year',\n",
    "        dcc.Dropdown(\n",
    "                id='start-year-dropdown', clearable=False,\n",
    "                value=[curr_year], options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in range(first_year, curr_year + 1, 1)\n",
    "                ])]), \n",
    "        #End year\n",
    "        html.Label(['End Year',\n",
    "        dcc.Dropdown(\n",
    "                id='end-year-dropdown', clearable=False,\n",
    "                value=[curr_year], options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in range(first_year, curr_year + 1, 1)\n",
    "        ])]),\n",
    "        #Add Player Button\n",
    "        html.Button('Submit', id='submit-hitter', n_clicks=None, type = 'submit'),\n",
    "        #Clear Player info Button\n",
    "        html.Button('Clear Player Info', id='clear-player', n_clicks=None),\n",
    "        #Input Box for games\n",
    "        html.Label(['Games', dcc.Input(id='game_input', type='number', min=1, max=games, step=1)]),\n",
    "        #Label for Games Remaining\n",
    "        #HTML Table populated by DropDown; (Player, Years, Games)\n",
    "        html.Div(children = [f'Hitters Selected; Games Remaining: {games}'], id = 'game'),\n",
    "        html.Div(children = [generate_table(hit_sel, 'hit_sel')], id = 'hit_sel_tbl'),\n",
    "        #Clear lineup button\n",
    "        html.Button('Clear Lineup', id='clear-lineup', n_clicks=None, style = {'text-align': 'center'}),\n",
    "        #Multi DropDown for pitchers\n",
    "        html.Label([\n",
    "            \"Pitcher\",\n",
    "            dcc.Dropdown(\n",
    "                id='pitcher-dd-calc', clearable=True,\n",
    "                multi = False,\n",
    "                value=[], options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in ui_pitch_df['Name'].unique()\n",
    "                ])\n",
    "        ]),\n",
    "        #Start year\n",
    "        html.Label([ 'Start Year',\n",
    "        dcc.Dropdown(\n",
    "                id='start-year-dropdown-p', clearable=False,\n",
    "                value=[curr_year], options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in range(first_year, curr_year + 1, 1)\n",
    "                ])]), \n",
    "        #End year\n",
    "        html.Label(['End Year',\n",
    "        dcc.Dropdown(\n",
    "                id='end-year-dropdown-p', clearable=False,\n",
    "                value=[curr_year], options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in range(first_year, curr_year + 1, 1)\n",
    "        ])]),\n",
    "        #Add Pitcher Button\n",
    "        html.Button('Submit', id='submit-pitcher', n_clicks=None),\n",
    "        #Clear Pitcher info Button\n",
    "        html.Button('Clear Pitcher Info', id='clear-pitcher', n_clicks=None),\n",
    "        #Input Box for innings\n",
    "        html.Label(['Innings', dcc.Input(id='inn_input', type='number', min=1, max=innings, step=1)]),\n",
    "        #Label for Innings Remaining\n",
    "        #HTML Table populated by DropDown; (Player, Years, Innings)\n",
    "        html.Div(children = [f'Pitchers Selected; Games Remaining: {innings}'], id = 'inn'),\n",
    "        html.Div(children = [generate_table(pit_sel, 'pit_sel')], id = 'pit_sel_tbl'),\n",
    "        #Clear rotation button\n",
    "        html.Button('Clear Rotation', id='clear-rotation', n_clicks=None, style = {'text-align': 'center'}),\n",
    "        #Submit Buttom that is only clickable when innings and games remaining are 0\n",
    "        html.Button('Submit Team', id='sub-team', n_clicks=None, style = {'margin-left': '55px'}),\n",
    "        html.Div(children = ['Wins: '], id = 'team-wins-prediction', style={'margin-top': '20px', 'margin-left': '10px'}),\n",
    "        ], style = {'display': 'inline-block', 'margin-top': '100px', 'vertical-align': 'top'}),\n",
    "    #Creative Lineup Comparison Graph\n",
    "    html.Div(children = [\n",
    "        \n",
    "        dcc.Graph(id = 'team-wins', style={'width': '90vh', 'height': '90vh', 'text-align': 'center'}),\n",
    "        #Dropdowns for querying team wins graph\n",
    "        #Start year\n",
    "        html.Div(children = [\n",
    "        html.Div(\n",
    "            html.Label(['Start Year', \n",
    "            dcc.Dropdown(\n",
    "                    id='start-year-dropdown-g', clearable=False,\n",
    "                    value=[], options=[\n",
    "                        {'label': c, 'value': c}\n",
    "                        for c in range(first_year, curr_year + 1, 1)\n",
    "                    ])]), style = {'display': 'inline-block', 'width': '20%', 'margin-left': '55px'}), \n",
    "        #End year\n",
    "        html.Div(\n",
    "            html.Label(['End Year',\n",
    "            dcc.Dropdown(\n",
    "                    id='end-year-dropdown-g', clearable=False,\n",
    "                    value=[], options=[\n",
    "                        {'label': c, 'value': c}\n",
    "                        for c in range(first_year, curr_year + 1, 1)\n",
    "            ])]), style = {'display': 'inline-block', \"margin-left\": \"15px\", 'width': '20%'}),\n",
    "        html.Div(\n",
    "            html.Label([\n",
    "                \"Team\",\n",
    "                dcc.Dropdown(\n",
    "                    id='team-graph', clearable=True,\n",
    "                    multi=True,\n",
    "                    value=[], options=[\n",
    "                        {'label': c, 'value': c}\n",
    "                        for c in ui_hit_df['Team'].unique()\n",
    "                    ])\n",
    "            ]), style = {'display': 'inline-block', \"margin-left\": \"15px\", 'width': '20%'}),\n",
    "        html.Div(\n",
    "            html.Label([ 'Stat',\n",
    "            dcc.Dropdown(\n",
    "                    id='stat-dd', clearable=False,\n",
    "                    value=[], options=[\n",
    "                        'wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG'\n",
    "                    ])]), style = {'display': 'inline-block', \"margin-left\": \"15px\", 'width': '20%'}),\n",
    "        ], style = {'display': 'inline-block', 'width': '100%'})\n",
    "\n",
    "        ], style = {'display': 'inline-block', \"margin-left\": \"50px\"}),\n",
    "\n",
    "    #HITTER SECTION\n",
    "    # Team Dropdown for hitter table\n",
    "    html.Div(children = [\n",
    "    html.Label([\n",
    "        \"Team\",\n",
    "        dcc.Dropdown(\n",
    "            id='team-hit', clearable=True,\n",
    "            multi=True,\n",
    "            value=[], options=[\n",
    "                {'label': c, 'value': c}\n",
    "                for c in ui_hit_df['Team'].unique()\n",
    "            ], style = {'width': '25%'})\n",
    "    ]),\n",
    "    # Hitter Dropdown\n",
    "    html.Label([\n",
    "        \"Hitter\",\n",
    "        dcc.Dropdown(\n",
    "            id='hitter-dropdown', clearable=True,\n",
    "            multi = True,\n",
    "            value=[], options=[\n",
    "                {'label': c, 'value': c}\n",
    "                for c in ui_hit_df['Name'].unique()\n",
    "            ], style = {'width': '25%'})\n",
    "    ]),\n",
    "    #hitter research table\n",
    "    dash_table.DataTable(\n",
    "       data=ui_hit_df.to_dict('records'), ####### inserted line\n",
    "       columns = [{'id': c, 'name': c} for c in ui_hit_df.columns], ####### inserted line\n",
    "        id='htable',\n",
    "        filter_action='native',\n",
    "        row_selectable='single',\n",
    "        editable=False,\n",
    "        sort_action=\"native\",\n",
    "        sort_mode=\"multi\",\n",
    "        column_selectable=\"single\",\n",
    "        row_deletable=True,\n",
    "        selected_columns=[],\n",
    "        selected_rows=[],\n",
    "        page_action=\"native\",\n",
    "        page_current= 0,\n",
    "        page_size= 10,\n",
    "        hidden_columns = ['AB', 'TB']\n",
    "    )], style = {'display': 'inline-block'}),\n",
    "    #PITCHER SECTION\n",
    "    # Team Dropdown for pitcher table\n",
    "    html.Label([\n",
    "        \"Team\",\n",
    "        dcc.Dropdown(\n",
    "            id='team-pitch', clearable=True,\n",
    "            multi=True,\n",
    "            value=[], options=[\n",
    "                {'label': c, 'value': c}\n",
    "                for c in ui_pitch_df['Team'].unique()\n",
    "            ], style = {'width': '25%'})\n",
    "    ]),\n",
    "    # Pitcher Dropdown\n",
    "    html.Label([\n",
    "        \"Pitcher\",\n",
    "        dcc.Dropdown(\n",
    "            id='pitcher-dropdown', clearable=True,\n",
    "            multi = True,\n",
    "            value=[], options=[\n",
    "                {'label': c, 'value': c}\n",
    "                for c in ui_pitch_df['Name'].unique()\n",
    "            ], style = {'width': '25%'})\n",
    "    ]),\n",
    "    #pitcher research table\n",
    "    dash_table.DataTable(\n",
    "       data=ui_pitch_df.to_dict('records'), ####### inserted line\n",
    "       columns = [{'id': c, 'name': c} for c in ui_pitch_df.columns], ####### inserted line\n",
    "        id='ptable',\n",
    "        filter_action='native',\n",
    "        row_selectable='single',\n",
    "        editable=False,\n",
    "        sort_action=\"native\",\n",
    "        sort_mode=\"multi\",\n",
    "        column_selectable=\"single\",\n",
    "        row_deletable=True,\n",
    "        selected_columns=[],\n",
    "        selected_rows=[],\n",
    "        page_action=\"native\",\n",
    "        page_current= 0,\n",
    "        page_size= 10,\n",
    "        hidden_columns = ['HR']\n",
    "    )\n",
    "    \n",
    "])\n",
    "\n",
    "#HITTER RESEARCH SECTION CALLBACKS\n",
    "@app.callback(Output('htable', 'columns'), [Input('team-hit', 'value'), \n",
    "                                           Input('hitter-dropdown', 'value')])\n",
    "def update_columns(teams, hitters):\n",
    "    return [{\"name\": i, \"id\": i} for i in ui_hit_df.columns]\n",
    "    \n",
    "@app.callback(Output('htable', 'data'), [Input('team-hit', 'value'),\n",
    "                                        Input('hitter-dropdown', 'value')])\n",
    "def update_data(teams, hitters):\n",
    "  '''\n",
    "  Args: \n",
    "    teams: selected teams\n",
    "    htiters: selected hitters\n",
    "  '''\n",
    "  if teams and hitters:\n",
    "    a= hit_df.loc[(ui_hit_df.Team.isin(teams)) & (ui_hit_df.Name.isin(hitters))]\n",
    "    return a.to_dict('records')\n",
    "  elif teams:\n",
    "    a= hit_df.loc[(ui_hit_df.Team.isin(teams))]\n",
    "    return a.to_dict('records')\n",
    "  elif hitters:\n",
    "    a= hit_df.loc[(ui_hit_df.Name.isin(hitters))]\n",
    "    return a.to_dict('records')\n",
    "  return ui_hit_df.to_dict('records')\n",
    "\n",
    "#PITCHER RESEARCH SECTION CALLBACKS\n",
    "@app.callback(Output('ptable', 'columns'), [Input('team-pitch', 'value'), \n",
    "              Input('pitcher-dropdown', 'value')])\n",
    "def update_columns(teams, pitchers):\n",
    "    return [{\"name\": i, \"id\": i} for i in ui_pitch_df.columns]\n",
    "    \n",
    "@app.callback(Output('ptable', 'data'), [Input('team-pitch', 'value'),\n",
    "                                        Input('pitcher-dropdown', 'value')])\n",
    "def update_data(teams, pitchers):\n",
    "  '''\n",
    "  Args: \n",
    "    teams: selected teams\n",
    "    htiters: selected hitters\n",
    "  '''\n",
    "  if teams and pitchers:\n",
    "    a= ui_pitch_df.loc[(ui_pitch_df.Team.isin(teams)) & (ui_pitch_df.Name.isin(pitchers))]\n",
    "    return a.to_dict('records')\n",
    "  elif teams:\n",
    "    a= ui_pitch_df.loc[(ui_pitch_df.Team.isin(teams))]\n",
    "    return a.to_dict('records')\n",
    "  elif pitchers:\n",
    "    a= ui_pitch_df.loc[(ui_pitch_df.Name.isin(pitchers))]\n",
    "    return a.to_dict('records')\n",
    "  return ui_pitch_df.to_dict('records')\n",
    "\n",
    "#CALLBACK FOR PLAYER SUBMISSION\n",
    "@app.callback([Output('hit_sel_tbl', 'children'), Output('game', 'children'),\n",
    "               Output('sel_tbl', 'data'), Output('games_rem', 'data'),\n",
    "               Output('submit-hitter', 'n_clicks'), Output('clear-lineup', 'n_clicks'), \n",
    "               Output('game_input', 'max')],\n",
    "              [Input('hitter-dd-calc', 'value'), Input('start-year-dropdown', 'value'),\n",
    "              Input('end-year-dropdown', 'value'), Input('game_input', 'value'),\n",
    "              Input('submit-hitter', 'n_clicks'), Input('clear-lineup', 'n_clicks'),\n",
    "              State('sel_tbl', 'data'), State('games_rem', 'data')])\n",
    "def update_lineup(hitter, start_year, end_year, game_input, button, cl_button, sel_tbl, gs):\n",
    "    #clearing the lineup\n",
    "    if (cl_button):\n",
    "        hitters = pd.DataFrame(columns = ['Name', 'Years', 'Games'])\n",
    "        return generate_table(hitters, 'hit_sel'), 'Hitters Selected; Games Remaining: 1458', [], 1458, None, None, 1458 \n",
    "    if len(sel_tbl) == 0:\n",
    "        hitters = pd.DataFrame(columns = ['Name', 'Years', 'Games'])\n",
    "    else:\n",
    "        hitters = pd.DataFrame(sel_tbl['data-frame'])\n",
    "    if type(gs) == list:\n",
    "        gms = 1458\n",
    "    else:\n",
    "        gms = gs\n",
    "    if (hitter and start_year and end_year and game_input and button and (gms - game_input >= 0)):\n",
    "        years = f'{start_year} - {end_year}'\n",
    "        hitters = hitters.append({'Name': hitter, 'Years': years, 'Games': game_input}, ignore_index = True)\n",
    "        gms = gms - game_input\n",
    "    table = generate_table(hitters, 'hit_sel')\n",
    "    new_text = 'Hitters Selected; Games Remaining: ' + str(gms)\n",
    "    df = {'data-frame': hitters.to_dict('records')}\n",
    "    return (table, new_text, df, gms, None, None, gms)\n",
    "\n",
    "\n",
    "#CLEARING DROPDOWNS UPON PLAYER SUBMISSION\n",
    "@app.callback([Output('hitter-dd-calc', 'value'), Output('start-year-dropdown', 'value'),\n",
    "              Output('end-year-dropdown', 'value'), Output('game_input', 'value')],\n",
    "              [Input('clear-player', 'n_clicks')])\n",
    "def reset_dropdowns(button):\n",
    "    return None, None, None, None  \n",
    "\n",
    "#PITCHER SELECTION\n",
    "\n",
    "#CALLBACK FOR PLAYER SUBMISSION\n",
    "@app.callback([Output('pit_sel_tbl', 'children'), Output('inn', 'children'),\n",
    "               Output('psel_tbl', 'data'), Output('inn_rem', 'data'),\n",
    "               Output('submit-pitcher', 'n_clicks'), Output('clear-rotation', 'n_clicks'), \n",
    "               Output('inn_input', 'max')],\n",
    "              [Input('pitcher-dd-calc', 'value'), Input('start-year-dropdown-p', 'value'),\n",
    "              Input('end-year-dropdown-p', 'value'), Input('inn_input', 'value'),\n",
    "              Input('submit-pitcher', 'n_clicks'), Input('clear-rotation', 'n_clicks'),\n",
    "              State('psel_tbl', 'data'), State('inn_rem', 'data')])\n",
    "def update_rotation(pitcher, start_year, end_year, inn_input, button, cl_button, psel_tbl, inn):\n",
    "    #clearing the rotation\n",
    "    if (cl_button):\n",
    "        pitchers = pd.DataFrame(columns = ['Name', 'Years', 'Innings'])\n",
    "        return generate_table(pitchers, 'hit_sel'), 'Pitchers Selected; Innings Remaining: 1458', [], 1458, None, None, 1458 \n",
    "    if len(psel_tbl) == 0:\n",
    "        pitchers = pd.DataFrame(columns = ['Name', 'Years', 'Innings'])\n",
    "    else:\n",
    "        pitchers = pd.DataFrame(psel_tbl['data-frame'])\n",
    "    if type(inn) == list:\n",
    "        inns = 1458\n",
    "    else:\n",
    "        inns = inn\n",
    "    if (pitcher and start_year and end_year and inn_input and button and (inns - inn_input >= 0)):\n",
    "        years = f'{start_year} - {end_year}'\n",
    "        pitchers = pitchers.append({'Name': pitcher, 'Years': years, 'Innings': inn_input}, ignore_index = True)\n",
    "        inns = inns - inn_input\n",
    "    table = generate_table(pitchers, 'pit_sel')\n",
    "    new_text = 'Pitchers Selected; Innings Remaining: ' + str(inns)\n",
    "    df = {'data-frame': pitchers.to_dict('records')}\n",
    "    return (table, new_text, df, inns, None, None, inns)\n",
    "\n",
    "\n",
    "#CLEARING DROPDOWNS UPON PLAYER SUBMISSION\n",
    "@app.callback([Output('pitcher-dd-calc', 'value'), Output('start-year-dropdown-p', 'value'),\n",
    "              Output('end-year-dropdown-p', 'value'), Output('inn_input', 'value'),\n",
    "              Output('clear-pitcher', 'n_clicks')],\n",
    "              [Input('clear-pitcher', 'n_clicks')])\n",
    "def reset_p_dropdowns(button):\n",
    "      return None, None, None, None, None \n",
    "\n",
    "#UPDATING THE GRAPH BASED ON WHICH STAT TO DISPLAY, WHICH TEAM, AND WHICH YEAR\n",
    "@app.callback(Output('team-wins', 'figure'),\n",
    "              [Input('team-graph', 'value'), Input('start-year-dropdown-g', 'value'),\n",
    "              Input('end-year-dropdown-g', 'value'), Input('stat-dd', 'value'),\n",
    "              Input('team_stats', 'data'), Input('sub-team', 'n_clicks')])\n",
    "def update_figure(team, sy, ey, stat, team_stats, sub_team):\n",
    "    a = team_history.copy()\n",
    "    s = 'wRC+'\n",
    "    if team and sy and ey:\n",
    "        a = a.loc[(a.Team.isin(team)) & (a.Season >= sy) & (a.Season <= ey)]\n",
    "    elif sy and ey:\n",
    "        a = a.loc[(a.Season >= sy) & (a.Season <= ey)]\n",
    "    elif team:\n",
    "        a = a.loc[(a.Team.isin(team))]\n",
    "    if stat:\n",
    "        s = stat\n",
    "    a['my_team'] = False\n",
    "    if sub_team and team_stats:\n",
    "        team_stats['my_team'] = True\n",
    "        a = a.append(team_stats, ignore_index = True)\n",
    "    \n",
    "    fig = px.scatter(a, x = s, y = 'W', hover_data=('Team', 'Season', 'wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG'), \n",
    "                    color = 'my_team')\n",
    "    return fig\n",
    "\n",
    "#SUBMITTING A LINEUP\n",
    "@app.callback(Output('team_stats', 'data'), Output('team-wins-prediction', 'children'),\n",
    "              Input('sub-team', 'n_clicks'), State('psel_tbl', 'data'),\n",
    "              State('sel_tbl', 'data'), State('games_rem', 'data'),\n",
    "              State('inn_rem', 'data')\n",
    ")\n",
    "def submit_team(submit, pit_sel_tbl, hit_sel_tbl, gs, inn):\n",
    "    if submit is None:\n",
    "        raise PreventUpdate\n",
    "    reg_stats = {}\n",
    "    wins = ''\n",
    "    if submit and (gs == 0) and (inn == 0):\n",
    "        # convert hitters in hitter objects\n",
    "        hitters = np.array([])\n",
    "        for h in hit_sel_tbl['data-frame']:\n",
    "            # Parsing seasons string from hit_sel_tbl\n",
    "            seasons = h['Years']\n",
    "            first = int(seasons[:4])\n",
    "            second = int(seasons[-4:])\n",
    "            ##########\n",
    "            # checking to see that first is less than second\n",
    "            if (first <= second):\n",
    "                yr_range = list(range(first, second + 1, 1))\n",
    "            else:\n",
    "                yr_range = list(range(second, first + 1, 1))\n",
    "            player = hitter(h['Name'], yr_range, h['Games'])\n",
    "            hitters = np.append(hitters, player)\n",
    "        pitchers = np.array([])\n",
    "        for p in pit_sel_tbl['data-frame']:\n",
    "            # Parsing seasons string from hit_sel_tbl\n",
    "            seasons = p['Years']\n",
    "            first = int(seasons[:4])\n",
    "            second = int(seasons[-4:])\n",
    "            # checking to see that first is less than second\n",
    "            if (first <= second):\n",
    "                yr_range = list(range(first, second + 1, 1))\n",
    "            else:\n",
    "                yr_range = list(range(second, first + 1, 1))\n",
    "            player = pitcher(p['Name'], yr_range, p['Innings'])\n",
    "            pitchers = np.append(pitchers, player)\n",
    "\n",
    "        # hitters and pitchers should be full and contain enough info to make predictions\n",
    "        # make predictions\n",
    "        wins, reg_stats = wins_for_team(hitters, pitchers)\n",
    "        reg_stats['Team'] = 'my_team'\n",
    "        reg_stats['Season'] = 2022\n",
    "        reg_stats['W'] = wins\n",
    "        reg_stats['WAR_y'] = reg_stats.pop('WAR') * 162\n",
    "        reg_stats['Def'] = reg_stats['Def'] * 162\n",
    "        reg_stats['BsR'] = reg_stats['BsR'] * 162\n",
    "    return reg_stats, f'Wins: {wins}'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "-l7bXCd4Qvzw",
    "outputId": "41c5ef51-3d89-4f49-823e-cc0e5fde3b0d"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ4GGLzLSm_M"
   },
   "source": [
    "Barriers to progress: No traceback errors, no way to find value of local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "rss2G4GYeeCi",
    "outputId": "fdcb334d-9d75-4c57-86e0-62372c40e951",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.scatter(team_history, x = 'wRC+', y = 'W', hover_data=('Team', 'Season', 'wRC+', 'HR/9', 'BsR', 'WAR_y', 'Def', 'SLG'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jW8klHfKKmG",
    "R2pRlF2dKKmH",
    "u2hDzrOPUpCr",
    "EEwQsShZQ9Y4",
    "P3tCh3Rk1O0Q",
    "1tThlTTFRNVO",
    "UOfjo_O7bLi8",
    "dRsuQpGtKKmT",
    "zDtDEUA-JysX",
    "af87u7RkOJ1J",
    "lCfATNZoCoxA",
    "YzCFJA_bPKRd",
    "xG0Bi1uJy794",
    "rSdPSA3C9Hs9",
    "j5zBDkr3gOju",
    "k6dnlwOUKKmV",
    "11wJoCATbjSJ",
    "nAq3hjN3ZFok",
    "63Ai-PbwxuIQ"
   ],
   "name": "MLB-Build-a-Team.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "9547286dff2591aaaa7eaab979eddc94570581c9b2f48e4daa50cbd44f6283dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
